{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../data/assignment_preprocessed.csv\")\n",
    "dataset.drop(['id','price_per_sqm', 'agent_id'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical to numerical data**\n",
    "Can be done either with the use of mapping or with one-hot encoding. We will use mapping for the correlation analysis and one-hot encoding for the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_type_mapping = {'simple': 1, 'up': 2, 'premium': 3, 'star': 4}\n",
    "geo_mapping = {'northern sub': 1, 'south beach': 2, 'gentrification area': 3, 'beesy neighborhood': 4}\n",
    "floor_mapping = {'ground-floor': 0, 'basement': -1, 'mezzanine': 0.5, 'semi-basement': -0.5}\n",
    "subtype_mapping ={'apartment': 1, 'detached': 2, 'maisonette': 3, 'building': 4, 'villa': 5, 'studio': 6, 'other residential': 7, 'loft': 8, 'apartment complex': 9, 'bungalow': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['geography_name'] = dataset['geography_name'].map(geo_mapping).fillna(dataset['geography_name']).astype(int)\n",
    "dataset['ad_type'] = dataset['ad_type'].map(ad_type_mapping).fillna(dataset['ad_type']).astype(int)\n",
    "dataset['floor'] = dataset['floor'].map(floor_mapping).fillna(dataset['floor']).astype(float)\n",
    "dataset['subtype'] = dataset['subtype'].map(subtype_mapping).fillna(dataset['subtype']).astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to identify the most important attributes in predicting the price of a property"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_analysis(dataset, target_variable):\n",
    "    # calculate correlation matrix\n",
    "    corr_matrix = dataset.corr()\n",
    "\n",
    "    # get correlation with target variable\n",
    "    corr_with_target = corr_matrix[target_variable]\n",
    "\n",
    "    # sort correlation values in descending order\n",
    "    sorted_corr = corr_with_target.abs().sort_values(ascending=False)\n",
    "\n",
    "    # print top 10 attributes with highest correlation\n",
    "    return list(sorted_corr.iloc[1:11].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_analysis_list = correlation_analysis(dataset, 'price')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance using Decision Trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop(['price'], axis=1)\n",
    "y = dataset['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create decision tree model\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "# fit model to data\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# sort importances in descending order\n",
    "sorted_importances = importances.argsort()[::-1]\n",
    "\n",
    "# print top 10 attributes with highest importance\n",
    "decision_tree_list = list(X.columns[sorted_importances[:10]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=150)\n",
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': rf.feature_importances_})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_list = list(feature_importance.head(10)['Feature'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes that at least two of the ways found as an important feature\n",
    "important_attributes = list(set(correlation_analysis_list).intersection(decision_tree_list, random_forest_list) | set(decision_tree_list).intersection(random_forest_list) | set(correlation_analysis_list).intersection(random_forest_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../data/assignment_preprocessed.csv\")\n",
    "dataset.drop(['id','price_per_sqm', 'agent_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncode(df,colNames):\n",
    "    for col in colNames:\n",
    "        if( df[col].dtype == np.dtype('object')):\n",
    "            dummies = pd.get_dummies(df[col],prefix=col)\n",
    "            df = pd.concat([df,dummies],axis=1)\n",
    "\n",
    "            #drop the encoded column\n",
    "            df.drop([col],axis = 1 , inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_results(y_test, y_pred): # Evaluate model performance\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print('Mean squared error:', round(mse))\n",
    "    print('Root MSE:', round(math.sqrt(mse)))\n",
    "    print(\"RMSLE\",np.log(np.sqrt(mean_squared_error(y_test,y_pred))))\n",
    "    print('R2 score:', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There were {} columns before encoding categorical features'.format(dataset.shape[1]))\n",
    "encoded_dataset = oneHotEncode(dataset, list(dataset.select_dtypes(include=['object']).columns))\n",
    "print('There are {} columns after encoding categorical features'.format(encoded_dataset.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_important_attributes = list(dataset[important_attributes].select_dtypes(exclude=['object']).columns)\n",
    "\n",
    "for obj in list(dataset[important_attributes].select_dtypes(include=['object']).columns):\n",
    "    prefix = obj + '_'\n",
    "    encoded_important_attributes += [prefix + str(val) for val in dataset[obj].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X = encoded_dataset[encoded_important_attributes]\n",
    "y = encoded_dataset['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Random Forest Regression model\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "gb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict prices for test data\n",
    "y_pred = gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBModel = XGBRegressor()\n",
    "XGBModel.fit(X_train, y_train, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean absolute error on the validation data :\n",
    "y_pred = XGBModel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal',activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the network :\n",
    "model.compile(loss='mean_squared_logarithmic_error', optimizer='adam', metrics=['mse', 'mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train).astype(np.float32)\n",
    "y_train = np.asarray(y_train).astype(np.float32)\n",
    "X_test = np.asarray(X_test).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=250, batch_size=32, validation_split = 0.2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spitogatos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
